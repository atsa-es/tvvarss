---
title: Time-varying vector autoregressive state space (TVVARSS) modeling of multi-site community
  dynamics
author: "Eric Ward, Mark Scheuerell, Steve Katz"
date: "December 3, 2015"
output:
  html_document: default
  pdf_document:
    fig_caption: yes
---

## Overview
This file accompanies the Scheuerell et al. paper describing a new method for estimating time varying community interactions (e.g., predation, competition) from multivariate time series data. The general model framework follows from the original MAR(1) model of Ives et al. (2003), and subsequent state-space versions of Ives and Dakos (2012).

## Requirements
Our analyses require several packages not installed with base `R`, so we install them (if necessary) and then load them.

```{r, message=FALSE}
if(!require("reshape2")) {
    install.packages("reshape2")
    library("reshape2")
}
if(!require("R2jags")) {
    install.packages("R2jags")
    library("R2jags")
}
if(!require("RCurl")) {
    install.packages("RCurl")
    library("RCurl")
}
```


## Loading the raw data
We begin with the original data from Kenner et al. (2013), which are available from the Ecological Society of America Archives at

http://www.esapubs.org/archive/ecol/E094/244/

```{r}
# set URL
URL <- "http://www.esapubs.org/archive/ecol/E094/244/"
# get benthic algae/invert data
dat.bi <- read.csv(paste0(URL,"Benthic%20density%20raw%20data.csv"))
colnames(dat.bi) <- tolower(colnames(dat.bi))
# get benthic fish data
dat.bf <- read.csv(paste0(URL,"Benthic%20fish%20density%20raw%20data.csv"))
colnames(dat.bf) <- tolower(colnames(dat.bf))
# get midwater fish data
dat.mf <- read.csv(paste0(URL,"Midwater%20fish%20density%20raw%20data.csv"))
colnames(dat.mf) <- tolower(colnames(dat.mf))
```

For our purposes we want the mean density of each species by sampling occasion and location.

```{r}
# benthic algae/invert data
benthos <- aggregate(density ~ speciescode + station + period, data=dat.bi, "mean")
# combine bottom & midwater fish data
fish <- rbind(dat.bf,dat.mf)
# include juveniles and adults together
fish[,"density"] <- fish[,"adultdensity"] + fish[,"juvdensity"]
# fish data
fish2 <- aggregate(density ~ speciescode + station + period, data=fish, "mean")
# all data together
dat <- rbind(benthos,fish2)
```


## Setting up the guilds

***
**NOTES for this version**

1. We need the `RCurl` package to read from secure URL's.
2. We also need the token below to access the secure Gitub site; the token probably changes by IP address.
3. Once Github site is public, we won't need the token anymore.

```{r, message=FALSE}
if(!require("RCurl")) {
    install.packages("RCurl")
    library("RCurl")
}
```
***

We assign all of the species in the Kenner dataset to 1 of 16 guilds (see Table S? of the manuscript for a print version). We have saved the lookup table as a .csv file, which is available on the Github project site.

```{r}
# set URL
URL <- "https://raw.githubusercontent.com/eric-ward/TVVARSS/master/"
token <- "?token=AE0I0KvRgq2xRDsnNauXCnTjTqw73Cphks5Wp3pDwA%3D%3D"
fileName <- "species_sampled_lookup.csv"
# get LUT of guild names
guilds <- read.csv(textConnection(getURL(paste0(URL,fileName,token))))
colnames(guilds) <- tolower(colnames(guilds))
# assign guilds names
for(i in 1:dim(guilds)[1]) {
  dat[dat[,"speciescode"] %in% guilds[i,"speciescode"],"guild"] <- guilds[i,"guild"]
	}
```

There are two guilds that we do not want to include in our analysis because they are too rare in the dataset: 1) "large piscivores" (e.g., leopard shark, _Triakis semifasciata_), and 2) "pelagic piscivores" (i.e., jack mackerel, _Trachurus symmetricus_).

```{r}
# spp to drop/eliminate
spp.out <- c("Piscivorous fishes - pelagic","Large piscivorous fishes")
# drop spp/guilds of no interest
dat <- dat[!(dat$guild %in% spp.out),]
```


## Identifying the sampling stations

The original data were collected at 7 stations around San Nicolas, but we only model the dynamics of 6 of them. The seventh station, Sandy Cove, was added later in the study, is relatively deep compared to the other stations, and was surveyed with much less frequency. We assume that the data from the 6 stations are observations of 4 underlying "states", or realizations of the San Nicolas community (see table below). In the North region, we treat station 1 as a single observation of one state. In the West region, we treat stations 2 and 3 as samples from the same state. In the South region, we model 2 different states. Stations 4 and 5 are observations of a third state, and station 6 is from a fourth state.

```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl_1 <- "
| Station | Name              | Region | State |
|:-------:|:-----------------:|:------:|:-----:|
|    1    | Nav Fac           | North  |   1   |
|    2    | West End 1        | West   |   2   |
|    3    | West End 2        | West   |   2   |
|    4    | East Dutch Harbor | South  |   3   |
|    5    | West Dutch Harbor | South  |   3   |
|    6    | Daytona           | South  |   4   |
"
cat(tabl_1) # output the table in a format good for HTML/PDF/docx conversion
```

Now we drop station 7 and assign the remaining observations to their respective states.

```{r}
# drop station 7
dat <- dat[dat$station != 7,]
# assign obs to states
dat$state <- NA
dat$state[dat$station==1] <- 1
dat$state[dat$station==2 | dat$station==3] <- 2
dat$state[dat$station==4 | dat$station==5] <- 3
dat$state[dat$station==6] <- 4
```

## Data aggregation & transformation

Now we need to aggregate the data by guild and sampling station.

```{r}
dat.m <- aggregate(density ~ guild + station + period, data=dat, "sum")  		  
```

We will treat 0's in the aggregated data as NA's for two reasons:

1. They likely arise from imperfect detection or small sampling areas;
2. We need to use log-density, which would require some us to add some arbitrary, non-zero constant.

```{r}
dat.m[dat.m$density==0,"density"] <- NA
```

We need to use log-density to meet model assumptions, and we want to de-mean the data so that we do not need to estimate the means within the model.

```{r}
# log of density
dat.m$ldens <- log(dat.m$density)
# transform data to wide form (rows: time; cols: guilds x stations)
dat.m2 <- dcast(dat.m, period ~ guild + station, value.var="ldens")
# de-mean the data
dat.m2[,-1] <- scale(dat.m2[,-1],center=TRUE,scale=FALSE)
```

There are a few time periods with missing samples that we need to set to NA.

```{r}
# getting time periods with no samples (ie, NAs)
per.miss <- seq(max(dat.m2[,"period"]))[!(seq(max(dat.m2[,"period"])) %in% dat.m2[,"period"])]
# insert NAs for missing dates
dat.miss <- cbind(per.miss,matrix(NA,length(per.miss),ncol(dat.m2)-1))
colnames(dat.miss) <- colnames(dat.m2)
# concatenate the 2 data frames
dat.m2 <- rbind(dat.m2,dat.miss)
# re-order the data by time
dat.m2 <- dat.m2[order(dat.m2[,"period"]),]
```


## External drivers (covariates)

There are several external drivers known to affect food web dynamics in kelp forest ecosystems. In particular, we are interested in the potential roles of

1. predatory sea otters;
2. the El NiÃ±o - Southern Oscillation (ENSO) climate phenomenon;
3. commercial urchin harvest.

### 1. Otters
The sea otter data come from multiple sources. __NEED INFO HERE RE: EARLY SOURCE(S)__ ...and years from 1995-2011 come from Kenner et al. (2013).

```{r}
# get some otter data
```

### 2. ENSO
The ENSO data come from NOAA's National Centers for Environmental Information. Specifically, we used the sea-surface temperatures from the ENSO 3.4 zone. More information is available at

https://www.ncdc.noaa.gov/teleconnections/enso/

The ENSO data are recorded monthly, but the food web data were sampled twice per year (i.e., spring and autumn). Thus, we want to use ENSO signals indicative of the period preceding each of the spring and fall dates. Although those dates vary somewhat from year to year, we selected these months for each time period:

```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl_2 <- "
| Season | ENSO months                |
|:-------|:---------------------------|
| Spring | March, April, May          |
| Autumn | August, September, October |
"
cat(tabl_2) # output the table in a format good for HTML/PDF/docx conversion
```

### the old option
(Only goes back to 1982, however. Waiting to hear from Mantua on better choice; see below)
```{r}
# first year of data
yr.first <- 1980
# last year of data
yr.last <- 2011
# first set of months
mon1 <- c(3,4,5)
# second set of months
mon2 <- c(8,9,10)
# get all ENSO data
enso <- read.table("http://www.cpc.ncep.noaa.gov/data/indices/sstoi.indices", header=TRUE)
# trim data to correct years & index
nino34 <- enso[enso$YR>=yr.first & enso$YR<=yr.last,  c("YR","MON","NINO3.4")]
# assign months to periods (1=Spring; 2=Autumn)
nino34[nino34$MON %in% mon1,"period"] <- 1
nino34[nino34$MON %in% mon2,"period"] <- 2
# ts of period means
anom34 <- aggregate(NINO3.4 ~ period + YR, nino34, mean)[,c("YR","period","NINO3.4")]
# convert to z-score (i.e., a temperature anomoly)
anom34$NINO3.4 <- scale(anom34$NINO3.4)
colnames(anom34)[c(1,3)] <- c("year","anom34")
```

### the new option
```{r}
# first year of data
yr.first <- 1980
# last year of data
yr.last <- 2011
# first set of months
mon1 <- c(3,4,5)
# second set of months
mon2 <- c(8,9,10)
# get all ENSO data
filename <- "http://www.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/detrend.nino34.ascii.txt"
enso <- read.table(filename, header=TRUE)
# trim data to correct years & index
nino34 <- enso[enso$YR>=yr.first & enso$YR<=yr.last,  c("YR","MON","TOTAL")]
# assign months to periods
nino34[nino34$MON %in% mon1,"period"] <- 1
nino34[nino34$MON %in% mon2,"period"] <- 2
# ts of period means
anom34 <- aggregate(TOTAL ~ period + YR, nino34, mean)[,c("YR","period","TOTAL")]
# convert to z-score (i.e., a temperature anomoly)
anom34$TOTAL <- scale(anom34$TOTAL)
colnames(anom34)[c(1,3)] <- c("year","anom34")
```

### 3. Harvest



***
## Old stuff below here
***

Read in the raw data on the guild abundances. These are in log-space, and the few 0s in the dataset have been replaced with NAs, because species are likely present in low density (just not detected).

```{r}
spec.names = c("Abalone","Cleaner.fish","Giant.kelp","Herb.fish","Large.invert.eating.fish",
  "Limpets", "Omni.inverts", "Plankt.fish", "Pred.inverts", "Small.invert.eating.fish",   
  "Small.pisc.fish", "Snails", "Understory.kelp", "Urchins")
```

Number of sites, time steps, and species/guilds/groups:  
```{r}
nSites = 7
n <- dim(dat.m2)[1] # number of time points
m <- (dim(dat.m2)[2]-1)/nSites # number of guilds/ groups
```



```{r}
#Y1 = t(dat[,matrix(seq(1,m*m),m,m)[,4]]) # site 1, state 2
#Y2 = t(dat[,matrix(seq(1,m*m),m,m)[,1]]) # site 2, state 1
#Y3 = t(dat[,matrix(seq(1,m*m),m,m)[,2]]) # site 3, state 1
#Y4 = t(dat[,matrix(seq(1,m*m),m,m)[,5]]) # site 4, state 3
#Y5 = t(dat[,matrix(seq(1,m*m),m,m)[,6]]) # site 5, state 3
#Y6 = t(dat[,matrix(seq(1,m*m),m,m)[,7]]) # site 6, state 4
```

Now we'll read in the covariates. For this application, there are 3 covariates (ENSO, urchin harvest, otter counts), and 2 of the 3 have the same values for all regions becasue they are recorded at a coarser spatial scale than the sampling sites (ENSO, urchin harvest).  

```{r}
# covars = read.csv("covariates_by_region.csv")

# enso.wns = matrix(0,dim(covars)[1],m)
# otters.w = matrix(0,dim(covars)[1],m) # dd.1
# otters.n = matrix(0,dim(covars)[1],m) 
# otters.s = matrix(0,dim(covars)[1],m)
# for(i in 1:m) {
# 	enso.wns[,i] = covars[,"enso_W"]
# 	otters.w[,i] = covars[,"otter_W"]
# 	otters.n[,i] = covars[,"otter_N"]
# 	otters.s[,i] = covars[,"otter_S"]
# }

# For harvest, we're only including removals of urchins, hence
# 0s for other species
# harvest.wns = matrix(0,dim(covars)[1],m)
# harvest.wns[,14] = covars[,"harvest_W"]
```

## Data preparation
We need to vectorize the B matrix in our code, so we need to do some housekeeping to create some indices.
```{r}
m2 = m*m
rowIndices = rep(seq(1,m), m)
colIndices = sort(rowIndices)
Bindices = matrix(seq(1,m2),m,m)
Bprior = diag(m2)
BR = m2
Bz = rep(0,m2)
Bdiag = seq(1,m*m,by=(m+1)) # these are the indices of vecB for diagonal
Boffdiag = seq(1,m*m)[-Bdiag]
```

## The TVVARSS model in JAGS
```{r}
model = cat("

model {

  # B1 is interaction matrix
  B1.tau ~ dwish(Bprior, m2); # B1.tau is precision matrix of interactions
  B1.1[1:m2,1] ~ dmnorm(Bz,B1.tau);	# interactions at time 1

  # convert the B1 vec to a matrix for initial time step	
  for(cols in 1:m) {
    # go from vec space -> matrix
    Bmat.1[1:m,cols,1] <- B1.1[Bindices[1,cols]:Bindices[m,cols],1]; # this is by column            
  }

  # initial X0, or state of nature, varies by state, but shared prior
  X0.tau ~ dwish(Bprior[1:m,1:m], m); # prior precision matrix
  X.1[1:m,1] ~ dmnorm(Bz[1:m],X0.tau); # state 1
  X.2[1:m,1] ~ dmnorm(Bz[1:m],X0.tau); # state 2
  X.3[1:m,1] ~ dmnorm(Bz[1:m],X0.tau); # state 3
  X.4[1:m,1] ~ dmnorm(Bz[1:m],X0.tau); # state 4
				
  # process variance is independent/unequal by region
  # and independent/unequal across spp
  for(i in 1:m) {
    for(j in 1:3) {
      tauQ[i,j] ~ dgamma(0.01,0.01);
    }		
  }

  B1.offDiagTau ~ dgamma(0.001,0.001);
  B1.diagTau ~ dgamma(0.001,0.001);    

  # Priors for coefficients  
  for(i in 1:m) {
    CC[i,1] ~ dnorm(0,1); # effect of enso, varies by spp
    DD[i,1] ~ dnorm(0,1); # effect of otters, varies by spp
  }
  for(i in 1:13) {
    EE[i,1] <- 0; # effect of urchin harvest on non-urchin
  }
  EE[14,1] ~ dnorm(0,1); # effect of urchin harvest on urchins

  for(time in 2:n) {
    for(cols in 1:m) {
      # go from vec space -> matrix, but i think it's only way in jags
      Bmat.1[1:m,cols,time] <- B1.1[Bindices[1,cols]:Bindices[m,cols],time-1];    	   
    }
    	
    # calculate predicted state vector for states 1-4
    predX.1[1:m,time] <- Bmat.1[1:m,1:m,time-1] %*% (X.1[1:m,time-1]) + 
                         CC[1:m,1] * enso.wns[time-1,] +
                         DD[1:m,1] * otters.w[time-1,] +
                         EE[1:m,1] * harvest.wns[time-1,];#site 2/3

    predX.2[1:m,time] <- Bmat.1[1:m,1:m,time-1] %*% (X.2[1:m,time-1]) +
                         CC[1:m,1] * enso.wns[time-1,] +
                         DD[1:m,1] * otters.n[time-1,] +
                         EE[1:m,1] * harvest.wns[time-1,];#site 1
    	
    predX.3[1:m,time] <- Bmat.1[1:m,1:m,time-1] %*% (X.3[1:m,time-1]) +
                         CC[1:m,1] * enso.wns[time-1,] +
                         DD[1:m,1] * otters.s[time-1,] +
                         EE[1:m,1] * harvest.wns[time-1,];#site 4/5

    predX.4[1:m,time] <- Bmat.1[1:m,1:m,time-1] %*% (X.4[1:m,time-1]) +
                         CC[1:m,1] * enso.wns[time-1,] +
                         DD[1:m,1] * otters.s[time-1,] +
                         EE[1:m,1] * harvest.wns[time-1,];#site 6

    # include process variation - normally distributed errors
    # independent across spp and site
    for(spp in 1:m) {
      X.1[spp,time] ~ dnorm(predX.1[spp,time], tauQ[spp,1]);
      X.2[spp,time] ~ dnorm(predX.2[spp,time], tauQ[spp,2]);
      X.3[spp,time] ~ dnorm(predX.3[spp,time], tauQ[spp,3]);
      X.4[spp,time] ~ dnorm(predX.4[spp,time], tauQ[spp,3]);		    		
    }

  # update B0 and B1
    for(i in 1:(m*m-m)) {
      # off diagonal elements of B1 -- interactions
      B1.1[Boffdiag[i],time] ~ dnorm(B1.1[Boffdiag[i],time-1], B1.offDiagTau);	    		    	
    }
    for(i in 1:m) {
      # Diagonal elements of B1 -- density dep
      B1.1[Bdiag[i],time] ~ dnorm(B1.1[Bdiag[i],time-1], B1.diagTau);	    		    	
    }
  } # end time loop

  # observation / data model, assume R = diag and equal
  for(i in 1:m) {
    tauR[i] ~ dgamma(0.001,0.001);
  }	

  for(time in 1:n) {
    for(spp in 1:m) {
      Y2[spp,time] ~ dnorm(X.1[spp,time],tauR[spp]);
      Y3[spp,time] ~ dnorm(X.1[spp,time],tauR[spp]);
      Y1[spp,time] ~ dnorm(X.2[spp,time],tauR[spp]);
      Y4[spp,time] ~ dnorm(X.3[spp,time],tauR[spp]);
      Y5[spp,time] ~ dnorm(X.3[spp,time],tauR[spp]);
      Y6[spp,time] ~ dnorm(X.4[spp,time],tauR[spp]);																		
    }
  }
	
} # end JAGS model description

", file = "TVVARSS.txt")
```

