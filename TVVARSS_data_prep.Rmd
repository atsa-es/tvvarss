---
title: Time-varying vector autoregressive state space (TVVARSS) modeling of multi-site community
  dynamics
author: "Eric Ward, Mark Scheuerell, Steve Katz"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document:
    fig_caption: yes
---

## Overview
This file accompanies the Scheuerell et al. paper describing a new method for estimating time varying community interactions (e.g., predation, competition) from multivariate time series data. The general model framework follows from the original MAR(1) model of Ives et al. (2003), and subsequent state-space versions of Ives and Dakos (2012). Specifically, the TVVARSS model is 

$$\mathbf{y}_{i,t} = \mathbf{x}_{j,t} + \mathbf{v}_{i,t}$$

$$\mathbf{x}_{j,t} = \mathbf{B}_t \mathbf{x}_{j,t-1} + \mathbf{C} \mathbf{c}_{j,t} + \mathbf{w}_{j,t}$$

$$\mathbf{B}_t = \mathbf{B}_{t-1} + \mathbf{e}_{t}$$

The measured variates (in log-space) at station $i$ and time $t$ $\left(\mathbf{y}_{i,t}\right)$ are observations of some true, but unknown state $j$ at time $t$ ($\mathbf{x}_t$). The states at time $t$ are a function of intra- and inter-guild interactions $\left(\mathbf{B}_t \mathbf{x}_{j,t-1} \right)$, and environmental covariates $\left(\mathbf{C} \mathbf{c}_{j,t}\right)$. The interaction matrix $\mathbf{B}_t$ follows a random walk.

When setting up the data below, it will be important to keep in mind the dimensions of the vectors and matrices for the equations above. Specifically, for $m$ species/guilds at $n$ sites/stations, $q$ covariates, and $T$ time points, we have:

$\mathbf{y}_{i,t}$ is $\left[ m \times 1 \right]$ and hence $\mathbf{y}_{i}$ is $\left[ m \times T \right]$;

$\mathbf{x}_{j,t}$ is $\left[ m \times 1 \right]$ and hence $\mathbf{x}_{j}$ is $\left[ m \times T \right]$;

$\mathbf{B}_{t}$ is $\left[ m \times m \right]$;

$\mathbf{C}$ is $\left[ m \times q \right]$;

$\mathbf{c}_{j,t}$ is $\left[ q \times 1 \right]$ and hence $\mathbf{c}_{j}$ is $\left[ q \times T \right]$.


## Requirements
Our analyses require several packages not installed with base `R`, so we being by installing them (if necessary) and then loading them.

```{r, message=FALSE}
if(!require("reshape2")) {
    install.packages("reshape2")
    library("reshape2")
}
if(!require("R2jags")) {
    install.packages("R2jags")
    library("R2jags")
}
if(!require("RCurl")) {
    install.packages("RCurl")
    library("RCurl")
}
```


## Loading the raw data
We begin with the original data from Kenner et al. (2013), which are available from the Ecological Society of America Archives at

http://www.esapubs.org/archive/ecol/E094/244/

```{r}
# set URL
URL <- "http://www.esapubs.org/archive/ecol/E094/244/"
# get benthic algae/invert data
dat.bi <- read.csv(paste0(URL,"Benthic%20density%20raw%20data.csv"))
colnames(dat.bi) <- tolower(colnames(dat.bi))
# get benthic fish data
dat.bf <- read.csv(paste0(URL,"Benthic%20fish%20density%20raw%20data.csv"))
colnames(dat.bf) <- tolower(colnames(dat.bf))
# get midwater fish data
dat.mf <- read.csv(paste0(URL,"Midwater%20fish%20density%20raw%20data.csv"))
colnames(dat.mf) <- tolower(colnames(dat.mf))
```

For our purposes we want the mean density of each species by sampling occasion and location.

```{r}
# benthic algae/invert data
benthos <- aggregate(density ~ speciescode + station + period, data=dat.bi, "mean")
# combine bottom & midwater fish data
fish <- rbind(dat.bf,dat.mf)
# include juveniles and adults together
fish[,"density"] <- fish[,"adultdensity"] + fish[,"juvdensity"]
# fish data
fish2 <- aggregate(density ~ speciescode + station + period, data=fish, "mean")
# all data together
dat <- rbind(benthos,fish2)
```


## Setting up the guilds

***
**NOTES for this version**

1. We need the `RCurl` package to read from secure URL's.
2. We also need the token below to access the secure Gitub site; the token probably changes by IP address.
3. Once Github site is public, we won't need the token anymore.

***

We assign all of the species in the Kenner dataset to 1 of 16 guilds (see Table S? of the manuscript for a print version). We have saved the lookup table as a .csv file, which is available on the Github project site.

```{r}
# set URL
URL <- "https://raw.githubusercontent.com/eric-ward/TVVARSS/master/"
token <- "?token=AE0I0KvRgq2xRDsnNauXCnTjTqw73Cphks5Wp3pDwA%3D%3D"
fileName <- "species_sampled_lookup.csv"
# get LUT of guild names
guilds <- read.csv(textConnection(getURL(paste0(URL,fileName,token))))
colnames(guilds) <- tolower(colnames(guilds))
# assign guilds names
for(i in 1:dim(guilds)[1]) {
  dat[dat[,"speciescode"] %in% guilds[i,"speciescode"],"guild"] <- guilds[i,"guild"]
  }
```

There are two guilds that we do not want to include in our analysis because they are too rare in the dataset: 1) "large piscivores" (e.g., leopard shark, _Triakis semifasciata_), and 2) "pelagic piscivores" (i.e., jack mackerel, _Trachurus symmetricus_).

```{r}
# spp to drop/eliminate
spp.out <- c("Piscivorous fishes - pelagic","Large piscivorous fishes")
# drop spp/guilds of no interest
dat <- dat[!(dat$guild %in% spp.out),]
```


## Identifying the sampling stations

The original data were collected at 7 stations around San Nicolas, but we only model the dynamics of 6 of them. The seventh station, Sandy Cove, was added later in the study, is relatively deep compared to the other stations, and was surveyed with much less frequency. We assume that the data from the 6 stations are observations of 4 underlying "states", or realizations of the San Nicolas community (see table below). In the North region, we treat station 1 as a single observation of one state. In the West region, we treat stations 2 and 3 as samples from the same state. In the South region, we model 2 different states. Stations 4 and 5 are observations of a third state, and station 6 is from a fourth state.

```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl_1 <- "
| Station | Name              | Region | State |
|:-------:|:-----------------:|:------:|:-----:|
|    1    | Nav Fac           | North  |   1   |
|    2    | West End 1        | West   |   2   |
|    3    | West End 2        | West   |   2   |
|    4    | East Dutch Harbor | South  |   3   |
|    5    | West Dutch Harbor | South  |   3   |
|    6    | Daytona           | South  |   4   |
"
cat(tabl_1) # output the table in a format good for HTML/PDF/docx conversion
```

Now we drop station 7 and assign the remaining observations to their respective states.

```{r}
# drop station 7
dat <- dat[dat$station != 7,]
# assign obs to states
dat$state <- NA
dat$state[dat$station==1] <- 1
dat$state[dat$station==2 | dat$station==3] <- 2
dat$state[dat$station==4 | dat$station==5] <- 3
dat$state[dat$station==6] <- 4
```

## Data aggregation & transformation

Now we need to aggregate the data by guild and sampling station.

```{r}
dat.m <- aggregate(density ~ guild + station + period, data=dat, "sum")  		  
```

We will treat 0's in the aggregated data as NA's for two reasons:

1. They likely arise from imperfect detection or small sampling areas;
2. We need to use log-density, which would require some us to add some arbitrary, non-zero constant.

```{r}
dat.m[dat.m$density==0,"density"] <- NA
```

We need to use log-density to meet model assumptions, and we want to de-mean the data so that we do not need to estimate the means within the model.

```{r}
# log of density
dat.m$ldens <- log(dat.m$density)
# transform data to wide form (rows: time; cols: guilds x stations)
dat.m2 <- dcast(dat.m, period ~ guild + station, value.var="ldens")
# de-mean the data
dat.m2[,-1] <- scale(dat.m2[,-1],center=TRUE,scale=FALSE)
```

There are a few time periods with missing samples that we need to set to NA.

```{r}
# get time periods with no samples (ie, NAs)
per.miss <- seq(max(dat.m2[,"period"]))[!(seq(max(dat.m2[,"period"])) %in% dat.m2[,"period"])]
# insert NAs for missing dates
dat.miss <- cbind(per.miss,matrix(NA,length(per.miss),ncol(dat.m2)-1))
colnames(dat.miss) <- colnames(dat.m2)
# concatenate the 2 data frames
dat.m2 <- rbind(dat.m2,dat.miss)
# re-order the data by time
dat.m2 <- dat.m2[order(dat.m2[,"period"]),]
rownames(dat.m2) <- NULL
```

The data are now organized within one data frame, which is $\left[T \times (m \times n) \right]$, but we want our data to be $n$ different matrices that are each $\left[m \times T \right]$.

```{r}
# number of time points
TT <- dim(dat.m2)[1]
# number of stations
NN <- length(unique(dat$station))
# number of guilds
MM <- (dim(dat.m2)[2]-1)/NN
for(i in 1:NN) {
  assign(paste0("YY_",i),t(dat.m2[,seq(2,by=NN,length.out=MM)+(i-1)]))
}
```

The last thing we need to do here is define a few scalars and indices to use within the JAGS model below.

```{r}
M2 <- MM*MM
row_idx <- rep(seq(1,MM), MM)
BB_idx <- matrix(seq(1,M2),MM,MM)
BB_prior <- diag(M2)
BB_mean <- rep(0,M2)
BB_diag <- seq(1,M2,by=(MM+1)) # these are the indices of BB_vec for diagonal of BB_mat
BB_off <- seq(1,M2)[-BB_diag]
```



## External drivers (covariates)

There are several external drivers known to affect food web dynamics in kelp forest ecosystems. In particular, we are interested in the potential roles of 1) predatory sea otters; 2) the El Niño - Southern Oscillation (ENSO) climate phenomenon; and 3) commercial urchin harvest.

#### 1. Otters
The sea otter data come from multiple sources. __NEED INFO HERE RE: EARLY SOURCE(S)__ ...and years from 1995-2011 come from Kenner et al. (2013).

```{r}
# get some otter data
```

#### 2. El Niño / Southern Oscillation
We used the El Niño / Southern Oscillation (ENSO) index to capture large-scale environmental conditions around San Nicolas. Strong ENSO events are characterized by relatively warm water and intense winter storms that physically disturb benthic habitats. Specifically, we used the sea-surface temperatures (SST) from the ENSO 3.4 region of the tropical Pacific Ocean (for more information on ENSO, click [here]).

[here]: https://www.ncdc.noaa.gov/teleconnections/enso/enso-tech.php

The ENSO data are recorded monthly, but the food web data were sampled twice per year (i.e., spring and autumn). Thus, we want to use ENSO signals indicative of the period preceding each of the spring and fall dates. Although those dates vary somewhat from year to year, we selected these months for each time period:

```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl_2 <- "
| Season | ENSO months                |
|:-------|:---------------------------|
| Spring | March, April, May          |
| Autumn | August, September, October |
"
cat(tabl_2) # output the table in a format good for HTML/PDF/docx conversion
```

The SST data (and several other atmosphereic indices) are available from NOAA's [Climate Prediction Center](http://www.cpc.ncep.noaa.gov/data/indices/).

Let's download the file and trim away the information we don't need.

```{r}
# first year of data
yr.first <- 1980
# last year of data
yr.last <- 2011
# first set of months
mon1 <- c(3,4,5)
# second set of months
mon2 <- c(8,9,10)
# get all ENSO data
nino <- read.table("http://www.cpc.ncep.noaa.gov/data/indices/ersst4.nino.mth.81-10.ascii", header=TRUE)
# trim data to correct years & index
nino34 <- nino[nino$YR>=yr.first & nino$YR<=yr.last,  c("YR","MON","NINO3.4")]
# assign months to periods (1=Spring; 2=Autumn)
nino34[nino34$MON %in% mon1,"period"] <- 1
nino34[nino34$MON %in% mon2,"period"] <- 2
# ts of period means
anom34 <- aggregate(NINO3.4 ~ period + YR, nino34, mean)[,c("YR","period","NINO3.4")]
# convert to z-score (i.e., a temperature anomoly)
anom34$NINO3.4 <- scale(anom34$NINO3.4)
colnames(anom34)[c(1,3)] <- c("year","anom34")
# convert to [1 x TT] matrix for JAGS
enso <- matrix(anom34$anom34,nrow=1)
```

#### 3. Harvest

We obtained the commercial harvest (in metric tonnes) of sea urchins around San Nicolas Island from __GET CONTACT INFO__ . The harvest data are also on the Github project site.

```{r}
# set URL
URL <- "https://raw.githubusercontent.com/eric-ward/TVVARSS/master/"
token <- "?token=AE0I0Cg5QEMXImfUitSN4lYQLBvd27BTks5Wr-vvwA%3D%3D"
urchinFile <- "urchin_harvest_ts.csv"
# get LUT of guild names
urchins <- read.csv(textConnection(getURL(paste0(URL,urchinFile,token))))
# convert to [1 x TT] matrix for JAGS
harv <- matrix(urchins$sum,nrow=1)
```

```{r}
# covars = read.csv("covariates_by_region.csv")

# enso.wns = matrix(0,dim(covars)[1],MM)
# otters.w = matrix(0,dim(covars)[1],MM) # CC_ottr.1
# otters.n = matrix(0,dim(covars)[1],MM) 
# otters.s = matrix(0,dim(covars)[1],MM)
# for(i in 1:MM) {
#   enso.wns[,i] = covars[,"enso_W"]
# 	otters.w[,i] = covars[,"otter_W"]
# 	otters.n[,i] = covars[,"otter_N"]
# 	otters.s[,i] = covars[,"otter_S"]
# }

# For harvest, we're only including removals of urchins, hence
# 0s for other species
# harvest.wns = matrix(0,dim(covars)[1],MM)
# harvest.wns[,14] = covars[,"harvest_W"]
```

***
## Old stuff below here
***

```{r}
spec.names = c("Abalone","Cleaner.fish","Giant.kelp","Herb.fish","Large.invert.eating.fish",
  "Limpets", "Omni.inverts", "Plankt.fish", "Pred.inverts", "Small.invert.eating.fish",   
  "Small.pisc.fish", "Snails", "Understory.kelp", "Urchins")
```


## The TVVARSS model in JAGS
```{r}
model <- cat("

model {

  #--------
  # PRIORS
  #--------
  # BB is interaction matrix
  # precision matrix
  BB_tau ~ dwish(BB_prior, M2); 
  BB_tau_off ~ dgamma(0.001,0.001);
  BB_tau_diag ~ dgamma(0.001,0.001);    

  # initial X0, or state of nature, varies by state, but shared prior
  X0_tau ~ dwish(BB_prior[1:MM,1:MM], MM); # prior precision matrix
				
  # process variance is independent/unequal by region
  # and independent/unequal across spp
  for(i in 1:MM) {
    for(j in 1:3) {
      QQ_tau[i,j] ~ dgamma(0.01,0.01);
    }		
  }

  # Priors for coefficients  
  for(i in 1:MM) {
    CC_enso[i,1] ~ dnorm(0,0.001); # effect of enso, varies by spp
    CC_ottr[i,1] ~ dnorm(0,0.001); # effect of otters, varies by spp
  }
  for(i in 1:(MM-1)) {
    CC_harv[i,1] <- 0; # effect of urchin harvest on non-urchin
  }
  CC_harv[MM,1] ~ dnorm(0,0.001); # effect of urchin harvest on urchins

  #------------
  # LIKELIHOOD
  #------------
  # first time step
  BB_vec[1:M2,1] ~ dmnorm(BB_mean,BB_tau);  # interactions at time 1
  # convert BB_vec to a matrix for initial time step  
  for(cols in 1:MM) {
    BB_mat[1:MM,cols,1] <- BB_vec[BB_idx[1,cols]:BB_idx[MM,cols],1];            
  }

  X_1[1:MM,1] ~ dmnorm(BB_mean[1:MM],X0_tau); # state 1
  X_2[1:MM,1] ~ dmnorm(BB_mean[1:MM],X0_tau); # state 2
  X_3[1:MM,1] ~ dmnorm(BB_mean[1:MM],X0_tau); # state 3
  X_4[1:MM,1] ~ dmnorm(BB_mean[1:MM],X0_tau); # state 4
  
  # time steps 2:TT
  for(time in 2:TT) {
    for(cols in 1:MM) {
      # go from vec space -> matrix, but i think it's only way in jags
      BB_mat[1:MM,cols,time] <- BB_vec[BB_idx[1,cols]:BB_idx[MM,cols],time];    	   
    }
    	
    # calculate predicted state vectors
    # state 1; from obs 1
    mu_X_1[1:MM,time] <- BB_mat[1:MM,1:MM,time] %*% (X_1[1:MM,time-1]) + 
                         CC_enso[1:MM,1] %*% enso[1,time-1] +
                         CC_ottr[1:MM,1] %*% ottr[2,time-1] +
                         CC_harv[1:MM,1] %*% harv[1,time-1];
    # state 2; from obs 2 & 3
    mu_X_2[1:MM,time] <- BB_mat[1:MM,1:MM,time] %*% (X_2[1:MM,time-1]) +
                         CC_enso[1:MM,1] %*% enso[1,time-1] +
                         CC_ottr[1:MM,1] %*% ottr[1,time-1] +
                         CC_harv[1:MM,1] %*% harv[1,time-1];
    # state 3; from obs 4 & 5
    mu_X_3[1:MM,time] <- BB_mat[1:MM,1:MM,time] %*% (X_3[1:MM,time-1]) +
                         CC_enso[1:MM,1] %*% enso[1,time-1] +
                         CC_ottr[1:MM,1] %*% ottr[3,time-1] +
                         CC_harv[1:MM,1] %*% harv[1,time-1];
    # state 4; from obs 6
    mu_X_4[1:MM,time] <- BB_mat[1:MM,1:MM,time] %*% (X_4[1:MM,time-1]) +
                         CC_enso[1:MM,1] %*% enso[1,time-1] +
                         CC_ottr[1:MM,1] %*% ottr[3,time-1] +
                         CC_harv[1:MM,1] %*% harv[1,time-1];

    # include process variation - normally distributed errors
    # independent across spp and site
    for(spp in 1:MM) {
      X_1[spp,time] ~ dnorm(mu_X_1[spp,time], QQ_tau[spp,1]);
      X_2[spp,time] ~ dnorm(mu_X_2[spp,time], QQ_tau[spp,2]);
      X_3[spp,time] ~ dnorm(mu_X_3[spp,time], QQ_tau[spp,3]);
      X_4[spp,time] ~ dnorm(mu_X_4[spp,time], QQ_tau[spp,3]);		    		
    }

    # update BB
    # off-diagonal elements of BB: interactions
    for(i in 1:(M2-MM)) {
      BB_vec[BB_off[i],time] ~ dnorm(BB_vec[BB_off[i],time-1], BB_tau_off);	    		    	
    }
    # diagonal elements of BB: density dependence
    for(i in 1:MM) {
      BB_vec[BB_diag[i],time] ~ dnorm(BB_vec[BB_diag[i],time-1], BB_tau_diag);	    		    	
    }
  } # end time loop

  # observation / data model; assume RR = diagonal and unequal
  for(i in 1:MM) {
    RR_tau[i] ~ dgamma(0.001,0.001);
  }	

  for(time in 1:TT) {
    for(spp in 1:MM) {
      Y_1[spp,time] ~ dnorm(X_1[spp,time],RR_tau[spp]);
      Y_2[spp,time] ~ dnorm(X_2[spp,time],RR_tau[spp]);
      Y_3[spp,time] ~ dnorm(X_2[spp,time],RR_tau[spp]);
      Y_4[spp,time] ~ dnorm(X_3[spp,time],RR_tau[spp]);
      Y_5[spp,time] ~ dnorm(X_3[spp,time],RR_tau[spp]);
      Y_6[spp,time] ~ dnorm(X_4[spp,time],RR_tau[spp]);																		
    }
  }
	
} # end JAGS model description

", file = "TVVARSS.txt")
```

## Running the model

```{r, message=FALSE, warning=FALSE, cache=TRUE}
jags_dat <- list("Y_1","Y_2","Y_3","Y_4","Y_5","Y_6",
                 "enso.wns", "otters.w", "otters.s", "otters.n", "harvest.wns",
                 "MM","M2","TT","BB_idx","BB_prior","BB_mean","BB_off","BB_diag")

jags_par <- c("BB_vec", "BB_tau_off", "BB_tau_diag", "CC_enso", "CC_ottr", "CC_harv",
              "X_1", "X_2", "X_3", "X_4", "QQ_tau", "RR_tau")

jags_mod <- list(data = jags_dat,
                 parameters.to.save = jags_par,
                 inits = NULL,
                 model.file = "TVVARSS.txt",
                 n.chains = as.integer(4),
                 n.burnin = as.integer(1.0e5),
                 n.thin = as.integer(50),
                 n.iter = as.integer(1.2e5),
                 DIC = TRUE)

# start timer
timer_start <- proc.time()

# fit the model in JAGS & store results
# jags_fit <- do.call(jags.parallel, jags_mod)

# stop timer
(run_time_in_min <- round(((proc.time()-timer_start)/60)["elapsed"], 0))
```

## Model diagnostics