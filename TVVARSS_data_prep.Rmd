---
title: Time-varying vector autoregressive state space (TVVARSS) modeling of multi-site community
  dynamics
author: "Eric Ward, Mark Scheuerell, Steve Katz"
date: "December 3, 2015"
output:
  html_document: default
  pdf_document:
    fig_caption: yes
---

## Overview
This file accompanies the Scheuerell et al. paper describing a new method for estimating time varying community interactions (e.g., predation, competition) from multivariate time series data. The general model framework follows from the original MAR(1) model of Ives et al. (2003), and subsequent state-space versions of Ives and Dakos (2012).

## Requirements
Our analyses require several packages not installed with base `R`, so we install them (if necessary) and then load them.

```{r, message=FALSE}
if(!require("reshape2")) {
    install.packages("reshape2")
    library("reshape2")
}
if(!require("R2jags")) {
    install.packages("R2jags")
    library("R2jags")
}
if(!require("RCurl")) {
    install.packages("RCurl")
    library("RCurl")
}
```


## Loading the raw data
We begin with the original data from Kenner et al. (2013), which are available from the Ecological Society of America Archives at

http://www.esapubs.org/archive/ecol/E094/244/

```{r}
# set URL
URL <- "http://www.esapubs.org/archive/ecol/E094/244/"
# get benthic algae/invert data
dat.bi <- read.csv(paste0(URL,"Benthic%20density%20raw%20data.csv"))
colnames(dat.bi) <- tolower(colnames(dat.bi))
# get benthic fish data
dat.bf <- read.csv(paste0(URL,"Benthic%20fish%20density%20raw%20data.csv"))
colnames(dat.bf) <- tolower(colnames(dat.bf))
# get midwater fish data
dat.mf <- read.csv(paste0(URL,"Midwater%20fish%20density%20raw%20data.csv"))
colnames(dat.mf) <- tolower(colnames(dat.mf))
```

For our purposes we want the mean density of each species by sampling occasion and location.

```{r}
# benthic algae/invert data
benthos <- aggregate(density ~ speciescode + station + period, data=dat.bi, "mean")
# combine bottom & midwater fish data
fish <- rbind(dat.bf,dat.mf)
# include juveniles and adults together
fish[,"density"] <- fish[,"adultdensity"] + fish[,"juvdensity"]
# fish data
fish2 <- aggregate(density ~ speciescode + station + period, data=fish, "mean")
# all data together
dat <- rbind(benthos,fish2)
```


## Setting up the guilds

***
**NOTES for this version**

1. We need the `RCurl` package to read from secure URL's.
2. We also need the token below to access the secure Gitub site; the token probably changes by IP address.
3. Once Github site is public, we won't need the token anymore.

```{r, message=FALSE}
if(!require("RCurl")) {
    install.packages("RCurl")
    library("RCurl")
}
```
***

We have assigned all of the species in the Kenner dataset to 1 of 16 guilds (see Table S? of the manuscript for a print version). We have saved the lookup table as a .csv file, which is available on the Github project site.

```{r}
# set URL
URL <- "https://raw.githubusercontent.com/eric-ward/TVVARSS/master/"
token <- "?token=AE0I0KvRgq2xRDsnNauXCnTjTqw73Cphks5Wp3pDwA%3D%3D"
fileName <- "species_sampled_lookup.csv"
# get LUT of guild names
guilds <- read.csv(textConnection(getURL(paste0(URL,fileName,token))))
colnames(guilds) <- tolower(colnames(guilds))
# assign guilds names
for(i in 1:dim(guilds)[1]) {
  dat[dat[,"speciescode"] %in% guilds[i,"speciescode"],"guild"] <- guilds[i,"guild"]
	}
```

There are two guilds that we do not want to include in our analysis because they are too rare in the dataset: 1) "large piscivores" (e.g., leopard shark, _Triakis semifasciata_), and 2) "pelagic piscivores" (i.e., jack mackerel, _Trachurus symmetricus_).

```{r}
# spp to drop/eliminate
spp.out <- c("Piscivorous fishes - pelagic","Large piscivorous fishes")
# drop spp/guilds of no interest
dat <- dat[!(dat$guild %in% spp.out),]
```


## Setting up sites

**At present**

The original data are from 7 sites, but we're only going to model the dynamics of 6 of those sites because the 7th (at the navy pier) has lots of missing data. For the remaining 6 sites, we're going to assume that there are 4 underlying "states"", or realizations of the San Nicolas community (see table below). In the North region, we're going to assume site 1 is a single observation of one state. In the West region, we're going to assume sites 2 and 3 are samples from the same state. In the South region, we're going to model 2 states, grouping sites 4 and 5, and keeping site 6 separate because of spatial location and differences in habitat.  

| Site | Name              | Region | State |
|:----:|:-----------------:|:------:|:-----:|
|   1	 | Nav Fac           | North  |   1   |
|   2  | Sandy Cove        | West   |   2   |
|   3  | West End          | West   |   2   |
|   4  | East Dutch Harbor | South  |   3   |
|   5  | West Dutch Harbor | South  |   3   |
|   6  | Daytona           | South  |   4   |

**OR**

The original data were collected at 7 sites around San Nicolas. However, we assume that there are only 4 underlying "states"", or realizations of the San Nicolas community (see table below). In the North region, we treat site 1 as a single observation of one state. In the West region, we treat sites 2, 3 & 7 as samples from the same state. In the South region, we model 2 different states, grouping sites 4 and 5, and keeping site 6 separate because of spatial location and differences in habitat.

| Site | Name              | Region | State |
|:----:|:-----------------:|:------:|:-----:|
|   1  | Nav Fac           | North  |   1   |
|   2  | West End 1        | West   |   2   |
|   3  | West End 2        | West   |   2   |
|   4  | East Dutch Harbor | South  |   3   |
|   5  | West Dutch Harbor | South  |   3   |
|   6  | Daytona           | South  |   4   |
|   7  | Sandy Cove        | West   |   2   |

```{r}
dat$state <- NA
dat$state[dat$station==1] <- 1
dat$state[dat$station==2 | dat$station==3 | dat$station==7] <- 2
dat$state[dat$station==4 | dat$station==5] <- 3
dat$state[dat$station==6] <- 4
```

##Data preparation

Now we need to aggregate the data by guild and sampling station.

```{r}
dat.m <- aggregate(density ~ guild + station + period, data=dat, "sum")  		  
```

We will treat 0's in the aggregated data as NA's for two reasons:

1. They likely arise from imperfect detection or small sampling areas;
2. We need to use log-density, which would require some us to add some arbitrary, non-zero constant.

```{r}
dat.m[dat.m$density==0,"density"] <- NA
```

We need to use log-density to meet model assumptions, and we want to de-mean the data so that we do not need to estimate them within the model.

```{r}
# log of density
dat.m$ldens <- log(dat.m$density)
# transform data to wide form (rows: time; cols: guilds x stations)
dat.m2 <- dcast(dat.m, period ~ guild + station, value.var="ldens")
# de-mean the data
dat.m2 <- scale(dat.m2,center=TRUE,scale=FALSE)
```

There are a few time periods with missing samples that we need to set to NA.

```{r}
# getting time periods with no samples (ie, NAs)
per.miss <- seq(max(dat.m2[,"period"]))[!(seq(max(dat.m2[,"period"])) %in% dat.m2[,"period"])]
# insert NAs for missing dates
dat.miss <- cbind(per.miss,matrix(NA,length(per.miss),ncol(dat.m2)-1))
colnames(dat.miss) <- colnames(dat.m2)
# concatenate the 2 data frames
dat.m2 <- rbind(dat.m2,dat.miss)
# re-order the data by time
dat.m2 <- dat.m2[order(dat.m2[,"period"]),]
```




***
***
***

Read in the raw data on the guild abundances. These are in log-space, and the few 0s in the dataset have been replaced with NAs, because species are likely present in low density (just not detected).

```{r, warning=FALSE, message=FALSE}
library(R2jags)
dat <- read.csv("biannual_var_flat_by_obs_zero.csv", header=TRUE, sep=",")
# Get rid of infinite values
for(i in 1:dim(dat)[2]) {
	dat[which(is.finite(dat[,i])==FALSE),i] = NA
}
# De-mean each of the individual time series
for(i in 1:dim(dat)[2]) {
	dat[,i] = dat[,i] - mean(dat[,i],na.rm=T)
}

spec.names = c("Abalone","Cleaner.fish","Giant.kelp","Herb.fish","Large.invert.eating.fish",
  "Limpets", "Omni.inverts", "Plankt.fish", "Pred.inverts", "Small.invert.eating.fish",   
  "Small.pisc.fish", "Snails", "Understory.kelp", "Urchins")
```

Number of sites, time steps, and species/guilds/groups:  
```{r}
nSites = 7
n <- dim(dat)[1] # number of time points
m <- dim(dat)[2]/nSites # number of guilds/ groups
```



```{r}
Y1 = t(dat[,matrix(seq(1,m*m),m,m)[,4]]) # site 1, state 2
Y2 = t(dat[,matrix(seq(1,m*m),m,m)[,1]]) # site 2, state 1
Y3 = t(dat[,matrix(seq(1,m*m),m,m)[,2]]) # site 3, state 1
Y4 = t(dat[,matrix(seq(1,m*m),m,m)[,5]]) # site 4, state 3
Y5 = t(dat[,matrix(seq(1,m*m),m,m)[,6]]) # site 5, state 3
Y6 = t(dat[,matrix(seq(1,m*m),m,m)[,7]]) # site 6, state 4
```

Now we'll read in the covariates. For this application, there are 3 covariates (ENSO, urchin harvest, otter counts), and 2 of the 3 have the same values for all regions becasue they are recorded at a coarser spatial scale than the sampling sites (ENSO, urchin harvest).  

```{r}
covars = read.csv("covariates_by_region.csv")

enso.wns = matrix(0,dim(covars)[1],m)
otters.w = matrix(0,dim(covars)[1],m) # dd.1
otters.n = matrix(0,dim(covars)[1],m) 
otters.s = matrix(0,dim(covars)[1],m)
for(i in 1:m) {
	enso.wns[,i] = covars[,"enso_W"]
	otters.w[,i] = covars[,"otter_W"]
	otters.n[,i] = covars[,"otter_N"]
	otters.s[,i] = covars[,"otter_S"]
}

# For harvest, we're only including removals of urchins, hence
# 0s for other species
harvest.wns = matrix(0,dim(covars)[1],m)
harvest.wns[,14] = covars[,"harvest_W"]
```

## Data preparation
We need to vectorize the B matrix in our code, so we need to do some housekeeping to create some indices.
```{r}
m2 = m*m
rowIndices = rep(seq(1,m), m)
colIndices = sort(rowIndices)
Bindices = matrix(seq(1,m2),m,m)
Bprior = diag(m2)
BR = m2
Bz = rep(0,m2)
Bdiag = seq(1,m*m,by=(m+1)) # these are the indices of vecB for diagonal
Boffdiag = seq(1,m*m)[-Bdiag]
```

## Writing the JAGS model
```{r}
model = cat("

model {

	# B1 is interaction matrix
	B1.tau ~ dwish(Bprior, m2); # B1.tau is precision matrix of interactions
	B1.1[1:m2,1] ~ dmnorm(Bz,B1.tau);	# interactions at time 1
		
	# convert the B1 vec to a matrix for initial time step	
  for(cols in 1:m) {
   # go from vec space -> matrix, but i think it's only way in jags
   Bmat.1[1:m,cols,1] <- B1.1[Bindices[1,cols]:Bindices[m,cols],1]; # this is by column            
  }

  # initial X0, or state of nature, varies by state, but shared prior
  X0.tau ~ dwish(Bprior[1:m,1:m], m); # prior precision matrix
	X.1[1:m,1] ~ dmnorm(Bz[1:m],X0.tau); # state 1
	X.2[1:m,1] ~ dmnorm(Bz[1:m],X0.tau); # state 2
	X.3[1:m,1] ~ dmnorm(Bz[1:m],X0.tau); # state 3
	X.4[1:m,1] ~ dmnorm(Bz[1:m],X0.tau); # state 4
					
	# process variance is independent/unequal by region
  # and independent/unequal across spp
	for(i in 1:m) {
		for(j in 1:3) {
		   tauQ[i,j] ~ dgamma(0.01,0.01);
		}		
	}
	
  B1.offDiagTau ~ dgamma(0.001,0.001);
  B1.diagTau ~ dgamma(0.001,0.001);    
  
  # Priors for coefficients  
	for(i in 1:m) {
		CC[i,1] ~ dnorm(0,1); # effect of enso, varies by spp
		DD[i,1] ~ dnorm(0,1); # effect of otters, varies by spp
	}
	for(i in 1:13) {
		EE[i,1] <- 0; # effect of urchin harvest on non-urchin
	}
	EE[14,1] ~ dnorm(0,1); # effect of urchin harvest on urchins
	    
  for(time in 2:n) {
  	for(cols in 1:m) {
  	   # go from vec space -> matrix, but i think it's only way in jags
  	   Bmat.1[1:m,cols,time] <- B1.1[Bindices[1,cols]:Bindices[m,cols],time-1];    	   
  	}
    	
    # calculate predicted state vector for states 1-4
    predX.1[1:m,time] <- Bmat.1[1:m,1:m,time-1] %*% (X.1[1:m,time-1]) + 
CC[1:m,1] * enso.wns[time-1,] + DD[1:m,1] * otters.w[time-1,] + 
EE[1:m,1]*harvest.wns[time-1,];#site 2/3

    predX.2[1:m,time] <- Bmat.1[1:m,1:m,time-1] %*% (X.2[1:m,time-1]) + 
CC[1:m,1] * enso.wns[time-1,] + DD[1:m,1] * otters.n[time-1,] + 
EE[1:m,1]*harvest.wns[time-1,];#site 1
    	
    predX.3[1:m,time] <- Bmat.1[1:m,1:m,time-1] %*% (X.3[1:m,time-1]) + 
CC[1:m,1] * enso.wns[time-1,] + DD[1:m,1] * otters.s[time-1,] + 
EE[1:m,1]*harvest.wns[time-1,];#site 4/5

    predX.4[1:m,time] <- Bmat.1[1:m,1:m,time-1] %*% (X.4[1:m,time-1]) + 
CC[1:m,1] * enso.wns[time-1,] + DD[1:m,1] * otters.s[time-1,] + 
EE[1:m,1]*harvest.wns[time-1,];#site 6

    # include process variation - normally distributed errors
    # independent across spp and site
    for(spp in 1:m) {
      X.1[spp,time] ~ dnorm(predX.1[spp,time], tauQ[spp,1]);
      X.2[spp,time] ~ dnorm(predX.2[spp,time], tauQ[spp,2]);
      X.3[spp,time] ~ dnorm(predX.3[spp,time], tauQ[spp,3]);
      X.4[spp,time] ~ dnorm(predX.4[spp,time], tauQ[spp,3]);		    		
    }
        
    # update B0 and B1
	  for(i in 1:(m*m-m)) {
	    # off diagonal elements of B1 -- interactions
	    B1.1[Boffdiag[i],time] ~ dnorm(B1.1[Boffdiag[i],time-1], B1.offDiagTau);	    		    	
	  }
	  for(i in 1:m) {
	    # Diagonal elements of B1 -- density dep
	    B1.1[Bdiag[i],time] ~ dnorm(B1.1[Bdiag[i],time-1], B1.diagTau);	    		    	
	  }
  } # end time loop

	# observation / data model, assume R = diag and equal
	for(i in 1:m) {
	tauR[i] ~ dgamma(0.001,0.001);
	}	

	for(time in 1:n) {
		for(spp in 1:m) {
			Y2[spp,time] ~ dnorm(X.1[spp,time],tauR[spp]);
			Y3[spp,time] ~ dnorm(X.1[spp,time],tauR[spp]);
			Y1[spp,time] ~ dnorm(X.2[spp,time],tauR[spp]);
			Y4[spp,time] ~ dnorm(X.3[spp,time],tauR[spp]);
			Y5[spp,time] ~ dnorm(X.3[spp,time],tauR[spp]);
			Y6[spp,time] ~ dnorm(X.4[spp,time],tauR[spp]);																		
		}
	}
	
}

", file = "TVVARSS.txt")
```

